#!/bin/bash
#
# oc-test creates ose pods on the ose-master node (defaults to localhost) and
# runs one or more of the specified storage tests.
#
# The storage specific test functions are named xxx_test(), eg. general_test()
# or nfs_test(). The general approach, in order to not need separate yaml files
# for each test's pod spec, is to supply the yaml as stdin to the:
# oc create -f command.  Eg:
#   cat <<END | oc create -f -
#     <yaml here...>
#   END
#
# The storage tests are loosley based on Scott Creeley's doc:
#   https://mojo.redhat.com/docs/DOC-1050225

# See usage() for syntax.
#

VERSION=0.95
VERIFY_ONLY=0 # false
MASTER="$HOSTNAME"
OC_PREFIX=''
BB_DIR='/usr/share/busybox'

# nfs vars
NFS_MOUNT='/opt/nfs'

# test vars
##ALL_TESTS='general,nfs,gluster,rbd' ##don't support general tests for now...
ALL_TESTS='nfs,gluster,rbd'
TESTS=''
##TESTS='general' # default ## there is no default test now...
##GENERAL_TEST=1 # true, default ## don't support general test for now...
##GENERAL_TEST=0 # false
GLUSTER_TEST=0 # false
NFS_TEST=0     # false
RBD_TEST=0     # false


## functions ##

function usage() {

  cat <<END

  oc-test - execute one or more ose storage tests

SYNTAX
  oc-test [test-name] [options]

DESCRIPTION
  Verifies the ose-master environment and, optionally, executes one or more
  storage tests on the ose-master server.

  test-name  A list of one or more tests to run. Expected values are:
               nfs,
               gluster,
               rbd (or ceph),
               all.
             More than one test is specified with a comma separator, eg.
             "nfs,gluster".

  --verify   Only test the ose enviromment but don't run any tests.
  --master <node>
             Hostname or ip of the ose-master node, default is local host
  --oc-prefix <path>
             An optional path prefix appended to all "oc" commands, eg.
             "/root/origin/_output/local/bin/linux/amd64" for local ose builds.
  --sgids <number-list>
             An optional list of 0 or more supplemental group IDs (separated by
             a comma only) to be prepended to the default SGID provided by OSE.
             This list of GIDs only applies to shared storage tests (gluster,
             nfs) since this storage type supports multiple users. For block
             storage (eg. ceph), use the --block-gid option. If --sgids is used,
             each created pod uses these GIDs in its securityContext spec. If
             omitted, the default is to use the first id in the supplemental
             group ID range defined for the current project. See:
               $ oc get ns <project> -o yaml  # for the various IDs
  --block-gid <number>
             An optional supplemental group ID to used as the FSGroup value for
             the created pod. This GID only applies to block storage tests
             (ceph). For shared storage (eg. nfs, gluster), use the --sgids
             option. If --block-gid is specified each created pod uses this GID 
             in its securityContext spec. If omitted, the default is to use the
             first id in the supplemental group ID range defined for the
             current project. See:
               $ oc get ns <project> -o yaml  # for the various IDs
  --nfs-server <hostname|ip>
             NFS server's hostname or ip. Required if performing the nfs tests,
             otherwise ignored.
  --gluster-nodes <node-list>
             A list of 2 or more gluster storage node IP addresses (comma
             separator), which become the ose endpoints. Hostnames are converted
             to IPs using getent, but if DNS or /etc/hosts is not setup
             correctly it is better to supply IPs. Required if performing the
             gluster tests, otherwise ignored. Eg.:
               192.168.122.21,192.168.122.22
  --gluster-vol <volName>
             The name of an existing gluster/RHS volume. Required if performing
             the gluster tests, otherwise ignored.
  --rbd-monitors <node-list>
             A list of 1 or more ceph Monitor IP addresses (comma separated).
             Hostnames are converted to IPs using getent, but if DNS or
             /etc/hosts is not setup correctly it is better to supply the IP.
             If the default port of 6789 is not used then :portNum must follow
             all IPs not using the default port.  Required if performing the 
             block-rbd tests, otherwise ignored. Eg:
               192.168.122.133:6788,192.168.122.134
  --ceph-secret64 <base64 value>
             The base-64 encoded secret string generated by running:
               $ ceph auth get-key client.admin | base64
             If omitted and the rbd test is requested and a provided ceph 
             monitor is reachable via ssh, then this value is calculated by the
             rbd test and is not required. The user may be prompted for the 
             monitor's password. It is ignore by all tests other than the
             ceph-rbd tests.
  --rbd-image <name>
             The name of the ceph-rbd image. If the rbd pool is not defaulted to
             "rbd" then the pool name is also required in the form of:
               <pool-name>/<image-name>
             Ignored for all tests other than the ceph-rbd tests.
  --version  Show the version string.
  -q         Suppress "continue?" prompts, skip test environment setup check on
             each ose-node (which also eliminates password prompts on those
             nodes), and reduce instructional output.

END

}

# output the list of tests with commas replaced by spaces so it is suitable
# for use as an array. Note: ceph and rbd are synonyms for the same test and
# therefore ceph is changed to rbd. Returns 1 for errors.
# $1= a list of 1 or more tests, comma separated.
function parse_tests() {

  local tests="${1/ceph/rbd}"; local test

  tests="${tests//,/ }"

  # validate known test names
  for test in $tests; do
     if [[ ! "$ALL_TESTS" =~ "$test" ]] ; then
       echo "ERROR: unknown test \"$test\""
       return 1
     fi
  done

  echo "$tests"
  return 0
}

# set globals to true for each test passed in. Assumes the tests arg contains
# only valid test names.
function set_test_flags() {

  local tests="$@"; local test

  for test in $tests; do
     case "$test" in
        #general)  ## don't support general tests for now...
          #GENERAL_TEST=1 # true
        #;;
        gluster)
          GLUSTER_TEST=1
        ;;
        nfs)
          NFS_TEST=1
        ;;
        rbd|ceph)
          RBD_TEST=1
        ;;
     esac

  done

  return 0
}

# returns 1 on errors, else 0.
function parse_cmd() {

  local errcnt=0
  local opts='q'
  local long_opts='version,master:,verify,oc-prefix:,nfs-server:,gluster-nodes:,gluster-vol:,sgids:,block-gid:,rbd-monitors:,ceph-secret64:,rbd-image:'

  # no args means just show usage
  (( $# == 0 )) && { usage; exit 0; }

  eval set -- "$(getopt -o "$opts" --long $long_opts -- $@)"

  while true; do
      case "$1" in
        -q)
          QUIET=1 # true
          shift; continue
        ;;

        --version)
          echo $VERSION
          exit 0
        ;;

        --verify)
          VERIFY_ONLY=1; # true
          shift; continue
        ;;

        --master)
          MASTER="$2"
          shift 2; continue
        ;;

        --sgids) # can be a list
          ARG_SGIDS="$2"
          shift 2; continue
        ;;

        --block-gid) # single number
          ARG_BLK_GID="$2"
          shift 2; continue
        ;;

        --oc-prefix)
          OC_PREFIX="$2"
          shift 2; continue
        ;;

        --nfs-server)
          NFS="$2"
          shift 2; continue
        ;;

        --gluster-nodes)
          GLUSTER_NODES="$2" #comma separated list
          shift 2; continue
        ;;

        --gluster-vol)
          GLUSTER_VOL="$2"
          shift 2; continue
        ;;

        --rbd-monitors)
          MONITORS="$2" #comma separated list if more than 1
          shift 2; continue
        ;;

        --ceph-secret64)
          CEPH_SECRET64="$2"
          shift 2; continue
        ;;

        --rbd-image)
          RBD_IMAGE="$2"
          shift 2; continue
        ;;

        --)
          shift; break
        ;;
      esac
  done

  # remaining arg should be a test name(s) (unless --verify)
  (( $# > 1 )) && {
    shift;
    usage;
    echo "Syntax error: unexpected command line arg: $@";
    return -1; }

  # --oc-prefix:
  if [[ -n "$OC_PREFIX" ]] ; then
    # oc prefix must end in a '/'
    [[ "$OC_PREFIX" != */ ]] && OC_PREFIX="$OC_PREFIX/"
    # make sure oc prefix exists as a dir on the master node
    if ! ssh $MASTER "[[ -d $OC_PREFIX ]]" ; then
      echo "ERROR: no directory named \"$OC_PREFIX\" on master node ($MASTER)"
      return 1
    fi
  fi

  # return if --verify
  (( VERIFY_ONLY )) && return 0 # test name(s) not expected and ignored

  # make sure a test name was requested
  (( $# == 0 )) && {
    usage;
    echo "Syntax error: expected a list of one or more test names";
    return -1; }

  TESTS="$1"; shift
  # handle "all" tests
  [[ "$TESTS" == 'all' ]] && TESTS="$ALL_TESTS"

  # handle a list of tests
  TESTS=($(parse_tests "$TESTS")) # array
  if (( $? != 0 )) ; then
    echo "${TESTS[@]}" # error msg
    return -1
  fi
  set_test_flags $TESTS
  NUM_TESTS=${#TESTS[@]}

  # --sgids: can be a list
  # nothing needed here, just leave it as a string with commas

  # --nfs-server:
  if (( NFS_TEST )) ; then
    if [[ -z "$NFS" ]] ; then
      echo "ERROR: missing nfs server argument"
      echo
      usage && return 1
    fi
  fi

  # --gluster-nodes:
  if (( GLUSTER_TEST )) ; then
    if [[ -z "$GLUSTER_NODES" ]] ; then
      echo "ERROR: missing list of gluster nodes"
      echo
      usage && return 1
    fi
    if [[ -z "$GLUSTER_VOL" ]] ; then
      echo "ERROR: missing the gluster volume"
      echo
      usage && return 1
    fi
    # convert hostnames to IPs, if possible
    GLUSTER_NODES=($(convert_host_to_ip ${GLUSTER_NODES//,/ })) # array
  fi

  # --rbd-monitors:
  if (( RBD_TEST )) ; then
    if [[ -z "$MONITORS" ]] ; then
      echo "ERROR: missing RBD monitor node(s)"
      echo
      usage && return 1
    fi
    if [[ -z "$RBD_IMAGE" ]] ; then
      echo "ERROR: missing the RBD image name"
      echo
      usage && return 1
    fi
    # convert hostnames to IPs, if possible
    MONITORS=($(convert_host_to_ip ${MONITORS//,/ })) # array
  fi

  return 0
}

# output the unique nodes from the list of nodes provided.
# $@=list of nodes.
function uniq_nodes() {

  local nodes=($@)
 
  printf '%s\n' "${nodes[@]}" | sort -u
}

# returns true (0) if at least one of the passed-in ids are within (inclusive)
# any of the pased-in range(s).
#   arg1=test-ids, arg2=range(s).
# test-ids is a simple list of 1 or more numeric gids separated by only a comma.
# Ranges are of the form: <startID>/<count> or <startID>-<endID> and a list of
# ranges are separated by a comma (no spaces). Eg.
#   range-1,range-2,...range-N
function id_in_range() {

  local test_ids=${1//,/ } # list
  local range="${2//,/ }"  # list
  local id; local r; local start; local end; local cnt; local ok

  for r in ${range[@]}; do
     if [[ $r == *"/"* ]] ; then # <start>/<count> form
       start=${r%/*}; cnt=${r#*/}; let end=start+cnt-1
     else # <start>-<end> form
       start=${r%-*}; end=${r#*-}
     fi

     # test if any test_ids are within this range
     ok=0 # assume not within range r
     for id in $test_ids; do
        (( id >= start && id <= end )) && { ok=1; break; } # in range
     done
     (( ok )) && return 0 # a gid is in range r
  done

  return 1 # none of the test_ids are within any of the ranges
}

# sets the SGIDs, SUIDs, OS_SGID, OS_SUID, USE_SGIDS, and USE_BLK_GID global
# variables to the values found in the project definition for the passed-in
# namespace. 
# Note: ID ranges have 2 formats: <start>/<cnt> or <start>-<end>
function get_supplemental_ids() {

  local ns="$1"
  local out; local id

  out="$(ssh $MASTER "${OC_PREFIX}oc get ns $ns -o yaml")"

  # SGIDs:
  SGIDs="$(grep 'openshift.io/sa.scc.supplemental-groups:' <<<"$out")"
  SGIDs="${SGIDs#*: }" # remove the property name
  id="${SGIDs%%,*}" # first range

  # get ose default sgid
  if [[ $id == *"/"* ]] ; then # <start>/<count> form
    OS_SGID=${id%/*} # first number in range
  else # <start>-<end> form
    OS_SGID=${id%-*} # first number in range
  fi

  # for shared storage, prepend the --sgids list, if provided, to the
  # ose default sgid
  USE_SGIDS=$OS_SGID # initial value
  if [[ -n "$ARG_SGIDS" ]] ; then
    # error if the supplied sgids are not w/in at least 1 of the ose ranges
    if ! id_in_range $ARG_SGIDS "$SGIDs" ; then
      echo "ERROR: supplied SGIDs of \"$ARG_SGIDS\" is not within the range of"
      echo "  supplemental group IDs defined for the \"$NS\" project."
      echo "  Openshift's SGID range is: \"$SGIDs\"."
      echo "  Either supply a different --sgids= value or change the"
      echo "  'openshift.io/sa.scc.supplemental-groups' 1st range using:"
      echo "    $ oc edit ns $NS"
      return 1
    fi
    # ok to prepend the user-supplied sgids to the ose default sgid
    USE_SGIDS="$ARG_SGIDS,$OS_SGID"
  fi

  # for block storage, if the user supplied --block-gid then validate that
  # value and use it. Else, use the default FSGroup id, which is the same as
  # the default shared SGID, which is the 1st number in the 1st range.
  # ose default sgid
  USE_BLK_GID=$OS_SGID # default
  if [[ -n "$ARG_BLK_GID" ]] ; then
    # error if the supplied gid is not w/in at least 1 of the ose ranges
    if ! id_in_range $ARG_BLK_GID "$SGIDs" ; then
      echo "ERROR: supplied block GID of \"$ARG_BLK_GID\" is not within the range"
      echo "  of supplemental group IDs defined for the \"$NS\" project."
      echo "  Openshift's SGID range is: \"$SGIDs\"."
      echo "  Either supply a different --block-gid= value or change the"
      echo "  'openshift.io/sa.scc.supplemental-groups' 1st range using:"
      echo "    $ oc edit ns $NS"
      return 1
    fi
    # ok to use the supplied block gid
    USE_BLK_GID=$ARG_BLK_GID
  fi

  # SUIDs:
  SUIDs="$(grep 'openshift.io/sa.scc.uid-range:' <<<"$out")"
  SUIDs="${SUIDs#*: }" # remove the property name
  id="${SUIDs%%,*}" # first range

  if [[ $id == *"/"* ]] ; then # <start>/<count> form
    OS_SUID=${id%/*} # first number in range
  else # <start>-<end> form
    OS_SUID=${id%-*} # first number in range
  fi

  return 0
}

# Convert the passed-in list of possible hostnames to ip addresses.
# Output the list with space delimiters so it is easily made into an array.
# Note: the list of hostnames may include :portNum after each IP/hostname,
#   which is preserved.
function convert_host_to_ip() {

  local list=($*) # array
  local ips=''; local node; local ip; local port

  # nested function: returns true (0) if the passed-in name matches an ip-addr.
  function is_ip_addr() {

    local octet='(25[0-5]|2[0-4][0-9]|[01]?[0-9]?[0-9])' # cannot exceed 255
    local ipv4="^$octet\.$octet\.$octet\.$octet$"

    [[ "$1" =~ $ipv4 ]] && return 0 # true
    return 1 # false
  }

  # main #

  for node in ${list[*]}; do
     # check for optional :port
     port=''
     if [[ "${node%:*}" != "$node" ]] ; then # port
       port="${node#*:}"; node="${node%:*}"
     fi

     if is_ip_addr $node ; then
       ips+="$node"
     else
       ip="$(getent hosts $node)" # uses dns or /etc/hosts
       if (( $? != 0 )) || [[ -z "$ip" ]] ; then
         ips+="$node"
       else
         ips+="${ip%% *}" # just ip-addr
       fi
     fi
     # remember port, if present
     [[ -n "$port" ]] && ips+=":$port"
     # append trailing space separator
     ips+=' '
  done

  echo "$ips"
  return 0
}

# echos the current namespace (=project) without any quotemarks.
function get_current_namespace() {

  local ns

  ns=($(ssh $MASTER "${OC_PREFIX}oc project")) # array
  ns=${ns[2]} # includes quotes
  ns=${ns%\"}; ns=${ns#\"} # remove enclosing quotes

  echo "$ns"
  return 0
}

# validate openshift env on passed-in node. Return 1 on error. Sets the global
# array OSE_NODES to include all ready/scheduable nodes. Echos a summary of the
# current ose environment.
function validate_ose_env() {

  local out; local not_ready_nodes; local node

  echo "*** Validating ose-master: \"$MASTER\"..."
  echo

  ssh $MASTER "${OC_PREFIX}oc login -u admin -p ignored"
  (( $? != 0 )) && return 1
  
  # oc get nodes
  out="$(ssh $MASTER "${OC_PREFIX}oc get nodes")"
  if grep -q NotReady <<<"$out" ; then
    echo -e "ERROR: 1 or more nodes are not ready:\n$out"
    return 1
  fi
  # remove header from nodes list
  out="$(tail -n +2 <<<"$out")"

  # set OSE_NODES to the list of nodes
  not_ready_nodes=($(grep -v Ready <<<"$out" |
                     cut -d' ' -f1)) # not scheduable
  OSE_NODES=($(grep Ready <<<"$out" | cut -d' ' -f1)) # only scheduable nodes

  # verify docker and docker-selinux are installed on all ose nodes (and master)
  for node in $(uniq_nodes ${OSE_NODES[@]} $MASTER); do
    out="$(ssh $node "rpm -q docker docker-selinux")"
    [[ -z "$out" ]] && {
      echo "ERROR: docker and docker-selinux not installed on node $node";
      return 1; }
    (( $(wc -l <<<"$out") == 2 )) && continue # have needed dockers installed
    if grep -v 'docker-selinux' <<<"$out"; then
      echo "ERROR: docker-selinux missing on node $node"
    else
      echo "ERROR: docker missing on node $node"
    fi
    return 1 # error
  done

  # NOTE: this HostDir test only applies to the general tests which are no
  #   longer supported.
  # oc get scc
  #out="$(ssh $MASTER "${OC_PREFIX}oc get scc")"
  #if awk '{print $4}' <<<"$out" | grep -q false ; then #HOSTDIR col has a false
    #echo "ERROR: HOSTDIR is false:"
    #echo "$out"
    #echo
    #echo "Use the \"oc edit scc {privileged|restricted}\" command and set"
    #echo "'allowHostDirVolumePlugin' to true."
    #return 1
  #fi

  echo "... validated"
  echo
  echo "============================================="
  echo " OSE master node   : $MASTER"
  echo " OSE nodes         : ${OSE_NODES[@]}"
  if (( ${#not_ready_nodes[@]} )) ; then
    echo "  ** these nodes are not scheduable:"
    echo "       ${not_ready_nodes[@]}"
  fi
  echo " Current project   : \"$NS\""
  echo "   User IDs        : $SUIDs"
  echo "   Group IDs       : $SGIDs"
  echo "   Default SGID    : $OS_SGID"
  [[ -n "$ARG_SGIDS" ]] &&
    echo " Additional SGIDs  : $ARG_SGIDS  (from cmd args)"
  [[ -n "$ARG_BLK_GID" ]] &&
    echo " Additional blk GID: $ARG_BLK_GID  (from cmd args)"
  echo " Shared storage pod's GID : $USE_SGIDS"
  echo " Blk storage pod's FSGroup: $USE_BLK_GID"
  echo "============================================="
  echo

  return 0
}

# deletes the passed-in pod.
function delete_pod() {

  local pod=$1
  local i; local err; local out; local TRIES=10; local SLEEP=3

  # delete the target pod
  echo "... deleting pod \"$pod\" (if it exists)..."
  ssh -q $MASTER "${OC_PREFIX}oc delete pod $pod >&/dev/null"

  # wait for the delete to complete...
  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pod $pod 2>&1")"
     err=$?
     if (( err != 0 )) && 
        grep -q " not found" <<<"$out" ; then # expected err
       return 0
     fi
     if (( err != 0 )) ; then # unexpected
       echo "$out"
       return 1
     fi
  done

  echo -e "ERROR: $TRIES tries attempted to delete pod \"$pod\":\n$out"
  return 1
}

# deletes the passed-in pvc (claim).
function delete_pvc() {

  local pvc=$1
  local i; local err; local out; local TRIES=3; local SLEEP=3

  # delete the target pvc
  echo "... deleting pvc \"$pvc\" (if it exists)..."
  ssh -q $MASTER "${OC_PREFIX}oc delete pvc $pvc >&/dev/null"

  # wait for the delete to complete...
  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pvc $pvc 2>&1")"
     err=$?
     if (( err != 0 )) && 
        grep -q " not found" <<<"$out" ; then # expected err
       return 0
     fi
     if (( err != 0 )) ; then # unexpected
       echo "$out"
       return 1
     fi
  done

  echo -e "ERROR: $TRIES tries attempted to delete pvc \"$pvc\":\n$out"
  return 1
}

# deletes the passed-in pv (Persistent Volume).
function delete_pv() {

  local pv=$1
  local i; local err; local out; local TRIES=15; local SLEEP=2

  # delete the target pv
  echo "... deleting pv \"$pv\" (if it exists)..."
  ssh -q $MASTER "${OC_PREFIX}oc delete pv $pv >&/dev/null"

  # wait for the delete to complete...
  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pv $pv 2>&1")"
     err=$?
     if (( err != 0 )) && 
        grep -q " not found" <<<"$out" ; then # expected err
       return 0
     fi
     if (( err != 0 )) ; then # unexpected
       echo "$out"
       return 1
     fi
  done

  echo -e "ERROR: $TRIES tries attempted to delete pv \"$pv\":\n$out"
  return 1
}

# deletes the passed-in endpoint.
function delete_endpoint() {

  local endpt="$1"
  local i; local err; local out; local TRIES=3; local SLEEP=2

  # delete the target endpoint
  echo "... deleting endpoint \"$endpt\" (if it exists)..."
  ssh -q $MASTER "${OC_PREFIX}oc delete endpoints $endpt >&/dev/null"

  # wait for the delete to complete...
  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get endpoints $endpt 2>&1")"
     err=$?
     if (( err != 0 )) && 
        grep -q " not found" <<<"$out" ; then # expected err
       return 0
     fi
     if (( err != 0 )) ; then # unexpected
       echo "$out"
       return 1
     fi
  done

  echo -e "ERROR: $TRIES tries attempted to delete endpoint \"$endpt\":\n$out"
  return 1
}

# deletes the passed-in secret.
function delete_secret() {

  local secret="$1"
  local i; local err; local out; local TRIES=3; local SLEEP=2

  # delete the target secret
  echo "... deleting secret \"$secret\" (if it exists)..."
  ssh -q $MASTER "${OC_PREFIX}oc delete secret $secret >&/dev/null"

  # wait for the delete to complete...
  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get secret $secret 2>&1")"
     err=$?
     if (( err != 0 )) && 
        grep -q " not found" <<<"$out" ; then # expected err
       return 0
     fi
     if (( err != 0 )) ; then # unexpected
       echo "$out"
       return 1
     fi
  done

  echo -e "ERROR: $TRIES tries attempted to delete secret \"$secret\":\n$out"
  return 1
}

# returns 0 if the passed-in pod's container is missing the passed-in mount
# type (eg. rbd, glusterfs).
#   $1=mount type, $2= pod-name
# Note: this function should only be called after the target pod as been
#   verified to be Running.
function verify_mount() {

  local mnt="$1"; local pod="$2"
  local out

  echo "... checking mount type \"$mnt\" for pod \"$pod\" ..."

  # first check mount inside container
  out="$(ssh $MASTER "${OC_PREFIX}oc exec $pod mount | grep $mnt 2>&1")"
  if (( $? == 0 )) && [[ -n "$out" ]] &&
     grep -q "$BB_DIR" <<<"$out" ; then
    return 0 # success
  fi

  echo -e "ERROR: $mnt mount missing in pod $pod:\n$out"
  return 1
}

# returns 0 if the passed-in pod's container has access to the standard
# busybox volume (dir). For now "access" is defined as both read and write.
# Args: $1=pod-name, $2=PASS|FAIL (expected result)
# Note: this function should only be called after the target pod as been
#   verified to be Running.
function verify_perms() {

  local pod="$1"; local pf=$2
  local out; local err; local file # name used for write tests
  # expt_pass=0 --> want pass, expt_pass=1 --> want fail
  local expt_pass=0 # assume test is expected to pass
  local err_verb='denied' # again, assume test is expected to pass

  # nested function to execute the passed-in command on $pod.
  # Returns 1 on errors.
  function pod_exec() {
    local cmd="$@"

    # execute $cmd in target pod/container (some trickery...)
    out="$(ssh $MASTER "
               ${OC_PREFIX}oc exec $pod -- sh -c 'echo \"\$($cmd 2>&1)\"'
        ")"
    # it appears that $? is 0 even with a perms denied error...
    if [[ -n "$out" ]] && 
       grep -q -E 'Permission denied|No such file' <<<"$out" ; then
      return 1
    fi
    
    return 0
  }

  # main #

  [[ "$pf" != 'PASS' ]] && {
    expt_pass=1; err_verb='allowed'; } # test is expected to fail

  echo "... checking perms of $BB_DIR on pod \"$pod\". Expect $pf ..."

  # exeute ls in target pod/container
  err=0
  pod_exec ls $BB_DIR || err=1
  (( err == expt_pass )) || {
    echo "ERROR: READ access $err_verb"; return 1; }

  # exeute touch in target pod/container
  err=0
  file="$BB_DIR/$pod-$(date '+%Y_%m_%d:%H_%M_%s')" # append pod-date:time
  pod_exec touch $file || err=1
  (( err == expt_pass )) || {
    echo "ERROR: WRITE(touch) access $err_verb"; return 1; }

  # rm the file
  err=0
  pod_exec rm $file || err=1
  (( err == expt_pass )) || {
    echo "ERROR: WRITE(rm) access $err_verb"; return 1; }

  return 0
}

# returns 0 if the passed-in pod is running, has the expected mount, and has
# the expected access (perms) to the busybox dir. Note: both the perms and
# mount checks are optional.
#  $1= pod-name,
#  $2= PASS|FAIL flag (optional),
#  $3= mount type (optional) -- nfs, glusterfs
function verify_new_pod() {

  local pod="$1"; local pf="$2"; local mnt_type="$3"
  local i; local MAX=90 # num tries, ceph/rbd is slow to mount!
  local SLEEP=3; local out

  echo "... checking pod \"$pod\" ..."

  # pod Running?
  for ((i=0; i<$MAX; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pod $pod 2>&1")"
     (( $? != 0 )) && {
       echo "$out"; return 1; }
     if awk '{print $2}' <<<"$out" | grep -q '1/1' ; then # READY column
       break # success
     fi
  done

  (( i >= MAX )) && {
    echo -e "ERROR: $(((MAX*SLEEP))) seconds waiting for pod \"$pod\" to start:\n$out";
    return 1; }

  # check mount?
  if [[ -n "$mnt_type" ]] ; then
    verify_mount $mnt_type $pod || {
      echo "ERROR: expected \"$mnt_type\" mount missing"; return 1; }
  fi

  # verify perms?
  if [[ -n "$pf" ]] ; then
    verify_perms $pod $pf || {
      echo "ERROR: permission test failed"; return 1; }
  fi

  return 0
}

# returns 0 if the passed-in PV is running.
function verify_new_pv() {

  local pv="$1"
  local i; local TRIES=60; local SLEEP=2; local out

  echo "... checking PV \"$pv\" ..."

  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pv $pv 2>&1")"
     (( $? != 0 )) && { 
       echo "$out"; return 1; }
     if awk '{print $5}' <<<"$out" | grep -q 'Available' ; then # STATUS column
       return 0 # success
     fi
  done

  echo -e "ERROR: $(((TRIES*SLEEP))) seconds waiting for PV \"$pv\" to start:\n$out"
  return 1
}

# returns 0 if the passed-in PVC is running.
function verify_new_pvc() {

  local pvc="$1"
  local i; local TRIES=30; local SLEEP=2; local out

  echo "... checking PVC \"$pvc\" ..."

  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get pvc $pvc 2>&1")"
     (( $? != 0 )) && { 
       echo "$out"; return 1; }
     if awk '{print $3}' <<<"$out" | grep -q 'Bound' ; then # STATUS column
       return 0 # success
     fi
  done

  echo -e "ERROR: $(((TRIES*SLEEP))) seconds waiting for PVC \"$pvc\" to start:\n$out"
  return 1
}

# returns 0 if the passed-in endpoint is running.
function verify_new_endpoint() {

  local endpt="$1"
  local i; local TRIES=3; local SLEEP=3; local out

  echo "... checking endpoint \"$endpt\" ..."

  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get endpoints $endpt 2>&1")"
     (( $? != 0 )) && { 
       echo "$out"; return 1; }
     grep -q "$endpt" <<<"$out" && return 0
  done

  echo -e "ERROR: $(((TRIES*SLEEP))) seconds waiting for endpoints \"$endpt\" to start:\n$out"
  return 1
}

# returns 0 if the passed-in secret is running.
function verify_new_secret() {

  local secret="$1"
  local i; local TRIES=3; local SLEEP=2; local out

  echo "... checking secret \"$secret\" ..."

  for ((i=0; i<$TRIES; i++)); do
     sleep $SLEEP
     out="$(ssh $MASTER "${OC_PREFIX}oc get secret $secret 2>&1")"
     (( $? != 0 )) && { 
       echo "$out"; return 1; }
     grep -q 'Opaque' <<<"$out" && return 0
  done

  echo -e "ERROR: $(((TRIES*SLEEP))) seconds waiting for secret \"$secret\" to start:\n$out"
  return 1
}

# returns 1 on errors. Checks that each ose node is setup to run the general
# tests (mostly the hostPath case). The biggest check here is to ensure that
# the correct selinux label is on the /opt/data dir on each node.
# $@=ose-nodes.
function verify_general_setup() {

  local nodes="$@"
  local node; local out; local perms; local z; local w
  local label='svirt_sandbox_file_t'

  echo "...checking that all OSE-nodes are setup for the general tests..."
  echo "   (May be prompted for your password on each node, possibly"
  echo "    multiple times)..."

  for node in $nodes; do
     echo
     echo "   *** on node: $node..."

     # selinux label set?
     out="$(ssh $node "ls -Zd /opt/data")"
     (( $? != 0 )) || [[ -z "$out" ]] && {
       echo -e "ERROR: cannot get selinux label for /opt/data:\n$out";
       return 1; }
     out=($out) # array
     perms="${out[0]}"; w=${perms:8:1} # other W bit
     z="${out[3]}" # se output
     z=(${z//:/ }); z="${z[2]}" # just the label

     # correct label for container access?
     [[ "$z" == "$label" && "$w" == 'w' ]] && continue # no perms issue, next

     # need to correct perms and/or label -- just do both for simplicity
     echo "... need to fix up /opt/data..."
     out="$(ssh $node "(chmod 777 /opt/data && 
                chcon -R -t $label /opt/data)" 2>&1)"
     (( $? != 0 )) && {
       echo -e "ERROR: cannot chmod 777 and chcon $label /opt/data:\n$out";
       return 1; }
  done

  echo "...all OSE-nodes are setup for the general tests"
  return 0
}

# returns 1 on errors. On each ose-node checks that nfs-util and rpcbind are
# installed, that the correct selinux labels are set, and that port 2049 on 
# the NFS server is reachable.
# On the NFs server /etc/exports is checked to make sure that the NFS mount is
# present.
# $@=ose-nodes.
function verify_nfs_setup() {

  local nodes="$@"
  local node; local out; local cnt
  local bools=(openshift_use_nfs virt_use_nfs) # array
  local bools_cnt=${#bools[@]}

  echo "...checking /etc/exports on the NFS server ($NFS)..."
  if ! ssh $NFS "grep -q ^$NFS_MOUNT /etc/exports" ; then
    echo "ERROR: \"$NFS_MOUNT\" missing in /etc/exports"
    return 1
  fi 

  echo "...checking that all OSE-nodes are setup for the NFS client..."
  echo "   (May be prompted for your password on each node, possibly"
  echo "    multiple times)..."

  for node in $nodes; do
     echo
     echo "   *** on node: $node..."
     # installed?
     out="$(ssh $node "rpm -q nfs-utils rpcbind")"
     (( $? != 0 )) && {
       echo "ERROR: cannot access $node via ssh"; return 1; }
     if grep ' is not installed' <<<"$out" ; then
       echo -e "ERROR: nfs-util and/or rpcbind are not installed:\n$out"
       return 1
     fi

     # selinux nfs related bools set on?
     out="$(ssh $node "getsebool ${bools[@]}")"
     (( $? != 0 )) || [[ -z "$out" ]] && {
       echo -e "ERROR: cannot get selinux nfs booleans:\n         ${bools[@]}";
       return 1; }
     cnt=$(grep -c ' on$' <<<"$out")
     if (( cnt != bools_cnt )) ; then
       echo -e "ERROR: only $cnt selinux bools are ON, expected $bools_cnt:\n$out"
       echo "         $ setsebool -P <bool-name> 1  #sets these bools"
       return 1
     fi

     # port 2049 reachable?
     out="$(ssh $node "which telnet 2>&1 && telnet $NFS 2049 <<<quit 2>&1")"
     # note $? seems to always be 1...
     if [[ -n "$out" ]] && grep -q "No route to host" <<<"$out" ; then
       echo -e "ERROR: cannot telnet to port 2049:\n$out"
       return 1
     fi
  done

  echo "...all OSE-nodes are setup for the NFS client"
  return 0
}

# returns 1 on errors. Checks that gluster-client is installed (we can't check
# for running), and that the correct selinux labels are set on all ose nodes.
# $@=ose-nodes.
function verify_gluster_setup() {

  local nodes="$@"
  local node; local out; local cnt
  local bools=(virt_sandbox_use_fusefs virt_use_fusefs) # array
  local bools_cnt=${#bools[@]}

  echo "...checking that all OSE-nodes are setup for the gluster client..."
  echo "   (May be prompted for your password on each node, possibly"
  echo "    multiple times)..."

  for node in $nodes; do
     echo
     echo "   *** on node: $node..."
     # installed?
     out="$(ssh $node "rpm -q glusterfs-fuse")"
     if (( $? != 0 )) || [[ -z "$out" ]] ||
        grep ' is not installed' <<<"$out" ; then
       echo "ERROR: glusterfs-fuse seems to not be installed"
       echo "       (or ssh $node error)"
       return 1
     fi

     # selinux fuse related bools set on?
     out="$(ssh $node "getsebool ${bools[@]}")"
     (( $? != 0 )) || [[ -z "$out" ]] && {
       echo -e "ERROR: cannot get selinux fuse booleans:\n         ${bools[@]}";
       return 1; }
     cnt=$(grep -c ' on$' <<<"$out")
     if (( cnt != bools_cnt )) ; then
       echo -e "ERROR: only $cnt selinux bools are ON, expected $bools_cnt:\n$out"
       echo "         $ setsebool -P <bool-name> 1  #sets these bools"
       return 1
     fi
  done

  echo "...all OSE-nodes are setup for the gluster client"
  return 0
}

# returns 1 on errors. Checks that ceph-common is installed (we can't check
# for running), and that the correct selinux labels are set on all ose nodes.
# Note: as of now it's not well understood which labels are required for ceph.
# $@=ose-nodes.
function verify_ceph_setup() {

  local nodes="$@"
  local node; local out; local cnt
  local bools=(X Y) # array
  local bools_cnt=${#bools[@]}

  echo "...checking that all OSE-nodes have ceph-common installed"
  echo "   (May be prompted for your password on each node, possibly"
  echo "    multiple times)..."

  for node in $nodes; do
     echo
     echo "   *** on node: $node..."
     # installed?
     out="$(ssh $node "rpm -q ceph-common")"
     if (( $? != 0 )) || [[ -z "$out" ]] ||
        grep ' is not installed' <<<"$out" ; then
       echo "ERROR: ceph-common seems to not be installed"
       echo "       (or ssh $node error)"
       return 1
     fi

     # selinux ceph-rbd related bools set on?
     ### unknown at this time, so this check is commented out
     #out="$(ssh $node "getsebool ${bools[@]}")"
     #(( $? != 0 )) || [[ -z "$out" ]] && {
       #echo -e "ERROR: cannot get selinux fuse booleans:\n         ${bools[@]}";
       #return 1; }
     #cnt=$(grep -c ' on$' <<<"$out")
     #if (( cnt != bools_cnt )) ; then
       #echo -e "ERROR: only $cnt selinux bools are ON, expected $bools_cnt:\n$out"
       #echo "         $ setsebool -P <bool-name> 1  #sets these bools"
       #return 1
     #fi
  done

  echo "...all OSE-nodes are setup for ceph"
  return 0
}

# If the passed-in gid is already a number then it is echo'd as-is and 0 is
# returned. Otherwise, the passed-in gid is converted to its corresponding 
# numeric value, via ssh to the passed-in target server, and the numeric
# value is output. Returns 1 and echos a generic error msg if getent returns
# an error. In this case the caller should set their gid var to "".
# $1=gid value, $2=remote server
function handle_gid_string() {

  local gid="$1"; local server="$2"
  local rtn="$gid"; local err=0

  if [[ ! $gid =~ ^[0-9]+$ ]] ; then # not all numeric
    # get the numeric id from the passed-in server
    rtn="$(ssh $server "getent group $gid | cut -d: -f3")"
    err=$?
  fi

  (( err == 0 )) && { echo "$rtn"; return 0; }

  # getent error
  echo "WARN: unable to convert the string GID \"$gid\" to its numeric"
  echo "  equivalent on server $server. OSE doesn't support string GIDs"
  echo "  in its ID range implementation."
  return 1
}

# run the "general" tests.
## NOTE: not currently supported.
function general_test() {

  local pod; local node

  if (( ! QUIET )) ; then
    cat <<END_GENERAL
*** General test suite ***"

    These baseline tests test that the SGIDs $USE_SGIDS works in emptyDir
    and in hostPath. No PVs or claims are used here.

    Each ose-node needs to be setup for Write access to the /opt/data dir.
    Even with 777 perms, the default container selinux labels will prevent 
    write access. Therefore, /opt/data needs the 'svirt_sandbox_file_t'
    label added, per node. This is done using:
      $ chcon -R -t svirt_sandbox_file_t /opt/data

END_GENERAL

    read -p "Press any key to continue..." -t120 out

    # check the general setup on the ose nodes
    verify_general_setup ${OSE_NODES[@]} || {
      echo "ERROR: cannot verify hostPath is setup correctly on all ose-nodes";
      return 1; }
  fi

  echo
  echo "----------"
  echo "General Test 1: baseline: busybox, emptyDir"
  echo "  (no supplementalGroups defined for this pod)"
  echo

  pod='general-pod1'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
  securityContext:
    privileged: false
END

  verify_new_pod $pod || {
    echo "ERROR: general test #1: see above pod \"$pod\" error"; return 1; }

  echo
  echo "----------"
  echo "General Test 2: busybox, hostPath, SGIDs: $USE_SGIDS"
  echo

  pod='general-pod2'
  delete_pod $pod

  # create /opt/data on all ose-nodes for this test
  echo
  echo "...checking that all OSE-nodes have the /opt/data directory..."
  echo "   (May be prompted for your password on each node)"
  for node in ${OSE_NODES[@]}; do
    # perms test includes write (touch), need all access for "other"
    ssh -q $node "mkdir -p /opt/data && chmod 777 /opt/data"
  done

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: bb-vol2
      mountPath: $BB_DIR
      readOnly: false
  securityContext:
    supplementalGroups: [$USE_SGIDS]
    privileged: false
  volumes:
  - name: bb-vol2
    hostPath:
      path: /opt/data
END

  verify_new_pod $pod PASS || {
    echo "ERROR: general test #2: see above pod \"$pod\" error"; return 1; }

  return 0
}

# run the "nfs" tests.
function nfs_test() {

  local pod; local pv; local pvc; local out; local ssh_ok
  local vol_perms; local vol_gid

  echo
  echo "*** NFS test suite ***"
  echo

  # show more info about running these tests...
  if (( ! QUIET )) ; then
    # get nfs vol mount perms from nfs server
    echo "Connecting to NFS server $NFS via ssh."
    echo "You may need to enter a password. (possibly more than once)"
    sleep 1
    echo

    out="$(ssh $NFS "ls -ld $NFS_MOUNT 2>&1")"
    if (( $? == 0 )) ; then
      out=($out) # array
      vol_perms="${out[0]}"
      vol_gid="${out[3]}"
      # convert string gid value (if it's a string) to numeric
      vol_gid="$(handle_gid_string $vol_gid $NFS)"
      (( $? != 0 )) && { echo "$vol_gid"; vol_gid=''; }
    else
      echo "WARN: cannot retrieve permissions on $NFS_MOUNT from NFS server"
      echo "  \"$NFS\" via ssh. Be aware of potential permission mismatches"
      echo "  between openshift supplemental groups and the NFS mount directory"
      echo
      echo "$out"
    fi

    cat <<END_NFS

    Busybox is run with a volume mounted to the NFS server: $NFS.
    Remember to open port 2049 on the NFS server. A good test for this is,
    from the openshift-master, "$MASTER", execute:
       $ telnet $NFS 2049  # ctrl-c to exit
    To open port 2049 execute (on the NFS server):
       $ iptables -I INPUT 1 -p tcp --dport 2049 -j ACCEPT

    Also, on the NFS server, edit /etc/exports to include /opt/nfs, eg:
       /opt/nfs *(rw,sync,no_root_squash)

    The group ID for the NFS export directory $NFS_MOUNT is:
       ${vol_gid:-<unavailable>}
    and permissions on this directory are:
       ${vol_perms:-<unavailable>}

    To edit the range of supplemental group IDs, on the openshift-master, use:
       $ oc edit ns $NS
    and change the 'openshift.io/sa.scc.supplemental-groups' range to
    include the NFS group ID for $NFS_MOUNT.  Also, use:
       $ oc get ns $NS -o yaml
    to see the values for various IDs in the "$NS" project.

    On the other hand, if it's ok to change the perms on $NFS_MOUNT to match
    openshift's range of groups, then execute (on $NFS):
       $ chgrp $OS_SGID /opt/nfs

    Note: it may necessary to restart NFS:
       $ systemctl restart rpcbind nfs

END_NFS

    read -p "Press any key to continue..." -t120 out

    # check the nfs setup on the ose nodes
    verify_nfs_setup ${OSE_NODES[@]} || {
      echo "ERROR: cannot verify that NFS is setup correctly on all ose-nodes";
      return 1; }
  fi

  echo
  echo "----------"
  echo "NFS Test 1: baseline: busybox, nfs plugin"
  echo "  (no supplementalGroups defined for this pod)"
  echo

  pod='nfs-pod1'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: nfs-vol1
      mountPath: $BB_DIR
      readOnly: false
  securityContext:
      privileged: false
  volumes:
  - name: nfs-vol1
    nfs:
      path: "$NFS_MOUNT"
      server: $NFS
      readOnly: false
END

  verify_new_pod $pod FAIL nfs || {
    echo "ERROR nfs test 1: see above pod \"$pod\" error"; return 1; }

  echo
  echo "----------"
  echo "NFS Test 2: busybox, nfs plugin, SGIDs: $USE_SGIDS"
  echo

  pod='nfs-pod2'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: nfs-vol2
      mountPath: $BB_DIR
      readOnly: false
  securityContext:
    supplementalGroups: [$USE_SGIDS]
    privileged: false
  volumes:
  - name: nfs-vol2
    nfs:
      path: "$NFS_MOUNT"
      server: $NFS
      readOnly: false
END

  verify_new_pod $pod PASS nfs || {
    echo "ERROR nfs test 2: see above pod \"$pod\" error"; return 1; }

  echo
  echo "----------"
  echo "NFS Test 3: busybox, PV, PVC, SGIDs: $USE_SGIDS"
  echo

  # a. create PV
  pv='nfs-pv'
  pvc='nfs-pvc'
  # delete the pvc before creating the pv. That way verify_new_pv() will see
  # the "Available" status, and not "Bound" due to the existing PVC binding to
  # the new PV before the script sees "Available"
  delete_pvc $pvc
  delete_pv $pv

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: PersistentVolume
metadata:
  name: $pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteMany
  nfs:
    path: $NFS_MOUNT
    server: $NFS
END

  verify_new_pv $pv || {
    echo "ERROR: nfs test #3: can't create PV"; return 1; }

  # b. create PVC
  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: $pvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
     requests:
       storage: 1Gi
END

  verify_new_pvc $pvc || {
    echo "ERROR: nfs test #3: can't create PVC"; return 1; }

  # c. create pod using claim
  pod='nfs-pod3'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: nfs-vol3
      mountPath: $BB_DIR
      readOnly: false
  securityContext:
    supplementalGroups: [$USE_SGIDS]
    privileged: false
  volumes:
  - name: nfs-vol3
    persistentVolumeClaim:
      claimName: $pvc
END

  verify_new_pod $pod PASS nfs || {
    echo "ERROR nfs test 3: pod $pod not Running"; return 1; }

  return 0
}

# run the "glusterfs" tests.
function gluster_test() {

  local pod; local node; local endpoints=''; local out
  local endpt='gluster-endpoints'; local pv; local pvc
  local mnt_perms; local mnt_gid

  echo
  echo "*** Gluster test suite ***"
  echo

  # show more info about running these tests...
  if (( ! QUIET )) ; then
    # get gluster vol mount perms
    node="${GLUSTER_NODES[0]}"
    echo "Connecting to $node via ssh. You may need to enter a password."
    sleep 1
    echo
    out="$(ssh $node "
            ( mnt=\$(mount |
                   grep 'fuse.glusterfs' |
                   grep $GLUSTER_VOL |
                   awk '{print \$3}') # set remote mnt var
              ls -ld \$mnt ) 2>&1
           ")"
    if (( $? == 0 )) ; then
      out=($out) # array
      mnt_perms="${out[0]}"
      mnt_gid="${out[3]}"
      # convert string gid value (if it's a string) to numeric
      mnt_gid="$(handle_gid_string $mnt_gid $node)"
      (( $? != 0 )) && { echo "$mnt_gid"; vol_gid=''; }
    else
      echo "WARN: cannot retrieve permissions for \"$GLUSTER_VOL\" from gluster"
      echo "  node $node via ssh. Be aware of potential permission mismatches"
      echo "  between openshift supplemental groups and the gluster mount."
      echo
      echo "$out"
    fi

    cat <<END_GLUSTER

    The supplied gluster storage nodes (endpoints) and the glusterfs plugin
    are tested using the busybox container to access the $GLUSTER_VOL volume.
    On one of the gluster nodes, eg. $node, ensure that gluster is
    running, the "$GLUSTER_VOL" volume is active, and the volume mount 
    has the correct permissions. Eg:
       $ gluster peer status
       $ gluster volume status $GLUSTER_VOL
       $ mount | grep glusterfs
       # if $GLUSTER_VOL is not displayed, then:
       $ mount -a # assuming the vol mount is present in /etc/fstab
       # if the vol mount is not in /etc/fstab, then add it, eg:
       ${GLUSTER_NODES[0]}:/$GLUSTER_VOL /mnt/glusterfs/$GLUSTER_VOL glusterfs _netdev 0 0
       
    The group ID for the "$GLUSTER_VOL" volume mount is:
       ${mnt_gid:-<unavailable>}
    and permissions on this volume's mount directory are:
       ${mnt_perms:-<unavailable>}

    To edit the range of supplemental group IDs, on "$MASTER", use:
       $ oc edit ns $NS
    and change the 'openshift.io/sa.scc.supplemental-groups' range to
    include the ${GLUSTER_VOL}'s mount's group ID.  Also, use:
       $ oc get ns $NS -o yaml
    to see the values for various IDs in the "$NS" project.

    On all of the OSE nodes make sure to:
      $ yum install glusterfs-client
      $ setsebool -P virt_sandbox_use_fusefs 1  # on, add the fusefs label
      $ setenforce 1 # keep selinux enforcing

    **NOTE: the setsebool command above. It is critical for enabling POSIX
            file access from the target containers!!

END_GLUSTER
  
    read -p "Press any key to continue..." -t120 out

    # check the gluster setup on the ose nodes
    verify_gluster_setup ${OSE_NODES[@]} || {
      echo "ERROR: cannot verify gluster is setup correctly on all ose-nodes";
      return 1; }
  fi

  echo
  echo "----------"
  echo "Gluster Test 1: baseline: busybox, glusterfs plugin"
  echo "  (no supplementalGroups defined for this pod)"
  echo

  # a. create endpoints
  delete_endpoint $endpt

  for node in ${GLUSTER_NODES[@]}; do # need newlines
     endpoints+="- addresses:"$'\n'
     endpoints+="  - ip: $node"$'\n'
     endpoints+="  ports:"$'\n'
     endpoints+="  - port: 1"$'\n' # ignored
     endpoints+="    protocol: TCP"$'\n'
  done

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Endpoints
apiVersion: v1
metadata:
  name: $endpt
subsets:
$endpoints
END

  verify_new_endpoint $endpt || {
    echo "ERROR: gluster test #1: can't create endpoints"; return 1; }

  # b. create pod
  pod='gluster-pod1'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: gluster-vol1
      mountPath: $BB_DIR
      readOnly: false
  securityContext:
      privileged: false
  volumes:
  - name: gluster-vol1
    glusterfs:
      endpoints: $endpt
      path: $GLUSTER_VOL
      readOnly: false
END

  verify_new_pod $pod FAIL glusterfs || {
    echo "ERROR gluster test 1: see above pod \"$pod\" error"; return 1; }

  echo
  echo "----------"
  echo "Gluster Test 2: busybox, glusterfs plugin, SGIDs: $USE_SGIDS"
  echo

  pod='gluster-pod2'
  delete_pod $pod
  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: gluster-vol2
      mountPath: $BB_DIR
      readOnly: false
  securityContext:
    supplementalGroups: [$USE_SGIDS]
    privileged: false
  volumes:
  - name: gluster-vol2
    glusterfs:
      endpoints: $endpt
      path: $GLUSTER_VOL
      readOnly: false
END

  verify_new_pod $pod PASS glusterfs || {
    echo "ERROR gluster test 2: see above pod \"$pod\" error"; return 1; }

  echo
  echo "----------"
  echo "Gluster Test 3: busybox, PV, PVC, SGIDs: $USE_SGIDS"
  echo

  # a. create PV
  pv='gluster-pv'
  pvc='gluster-pvc'
  delete_pvc $pvc
  delete_pv $pv

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: PersistentVolume
metadata:
  name: $pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteMany
  glusterfs:
    endpoints: $endpt
    path: $GLUSTER_VOL
    readOnly: false
END

  verify_new_pv $pv || {
    echo "ERROR: gluster test #3: can't create PV"; return 1; }

  # b. create PVC
  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: $pvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
     requests:
       storage: 1Gi
END

  verify_new_pvc $pvc || {
    echo "ERROR: gluster test #3: can't create PVC"; return 1; }

  # c. create pod using claim
  pod='gluster-pod3'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: gluster-vol3
      mountPath: $BB_DIR
      readOnly: false
  securityContext:
    supplementalGroups: [$USE_SGIDS]
    privileged: false
  volumes:
  - name: gluster-vol3
    persistentVolumeClaim:
      claimName: $pvc
END

  verify_new_pod $pod PASS glusterfs || {
    echo "ERROR gluster test 3: see above pod \"$pod\" error"; return 1; }

  return 0
}

# run the "ceph-rbd" tests.
function rbd_test() {

  local pod; local pv; local pvc; local out
  local mon; local monitors=''; local pool; local image
  local secret_name='ceph-secret'

  pool="${RBD_IMAGE%/*}" # pool name is optional
  [[ "$pool" == "$RBD_IMAGE" ]] && pool='rbd' # not specified, use default
  image="${RBD_IMAGE#*/}" # remove pool name if present

  echo
  echo "*** RBD test suite ***"
  echo

  # get ceph monitor base64 secret value if not supplied
  if [[ -z "$CEPH_SECRET64" ]] ; then
    mon="${MONITORS[0]}"; mon="${mon%:*}" # remove port if present
    echo "Calculating base64 ceph secret value..."
    echo "Connecting to $mon via ssh. You may need to enter a password."
    sleep 1

    out="$(ssh $mon "ceph auth get-key client.admin | base64")"
    if (( $? != 0 )) || [[ -z "$out" ]] ; then
      echo -e "ERROR: cannot calculate base64 secret on $mon\n$out"
      echo
      echo "     Execute: ceph auth get-key client.admin | base64"
      echo "     and re-run this script providing --ceph-secret64"
      return 1
    fi
    CEPH_SECRET64="$out"
    echo "Computed ceph secret: $CEPH_SECRET64"
  fi

  if (( ! QUIET )) ; then
    cat <<END_RBD

    Busybox is run with a volume mounted via the RBD plugin. The ceph user is
    currently hard-coded to "admin" and the file system is assumed to be ext4.
    (Both of these assumptions can easily be removed and added as script args.) 

    Ceph needs to be installed and running correctly. Here are a few tips to 
    perform on one of the monitors (eg. ${MONITORS[0]}):
      $ rbd create ceph-image -s 1024  # create the image
      $ rbd map ceph-image             # map image to the default ('rbd') pool
      $ rbd showmapped
      $ ls /dev/rbd*                   # see which /dev/rbd device is used
      $ mkfs.ext4 /dev/rbd0            # create the file system
      # provide the below output as the --ceph-secret64 value
      $ ceph auth get-key client.admin | base64

      # on each OSE worker node:
      $ yum install -y ceph-common

END_RBD
  
    read -p "Press any key to continue..." -t120 out

    # check the ceph setup on the ose nodes
    verify_ceph_setup ${OSE_NODES[@]} || {
      echo "ERROR: cannot verify ceph is setup correctly on all ose-nodes";
      return 1; }
  fi

  echo
  echo "----------"
  echo "RBD Test 1: baseline: busybox, ceph-rbd plugin"
  echo "  (no fsGroup defined for this pod)"
  echo

  # a. create ceph-secret
  delete_secret $secret_name

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Secret
metadata:
  name: $secret_name
data:
  key: $CEPH_SECRET64
END

  verify_new_secret $secret_name || {
    echo "ERROR: RBD test #1: can't create secret"; return 1; }

  # b. create monitors yaml
  for mon in ${MONITORS[@]}; do # need newlines
     monitors+="       - $mon"$'\n' # mon may include :port
  done

  # c. create pod with rbd plugin defined inline
  pod='rbd-pod1'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: rbd-vol1
      mountPath: $BB_DIR
      readOnly: false
  securityContext: #no fsGroup: defined
    privileged: false
  volumes:
  - name: rbd-vol1
    rbd: 
      monitors: 
$monitors
      pool: $pool
      image: $image
      user: admin
      secretRef: 
        name: $secret_name
      fsType: ext4
      readOnly: false
END

  verify_new_pod $pod PASS rbd || {
    echo "ERROR rbd test 1: see above pod \"$pod\" error"; return 1; }

  echo
  echo "----------"
  echo "RBD Test 2: busybox, ceph-rbd plugin, PV, PVC, FSGroup: $USE_BLK_GID"
  echo

  # a. create pv
  pv='rbd-pv'
  pvc='rbd-pvc'
  delete_pvc $pvc
  delete_pv $pv

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: PersistentVolume
metadata:
  name: $pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteMany
  rbd:
    monitors: 
$monitors
    pool: $pool
    image: $image
    user: admin
    secretRef: 
      name: $secret_name
    fsType: ext4
    readOnly: false
END

  verify_new_pv $pv || {
    echo "ERROR: rbd test #2: can't create PV"; return 1; }

  # b. create PVC
  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: $pvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
     requests:
       storage: 1Gi
END

  verify_new_pvc $pvc || {
    echo "ERROR: rbd test #2: can't create PVC"; return 1; }

  # c. create pod using claim
  pod='rbd-pod2'
  delete_pod $pod

  cat <<END | ssh $MASTER "${OC_PREFIX}oc create -f -"
apiVersion: v1
kind: Pod
metadata:
  name: $pod
  labels:
    name: $pod
spec:
  containers:
  - name: $pod
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - name: rbd-vol2
      mountPath: $BB_DIR
      readOnly: false
  securityContext:
    fsGroup: $USE_BLK_GID
    privileged: false
  volumes:
  - name: rbd-vol2
    persistentVolumeClaim:
      claimName: $pvc
END

  verify_new_pod $pod PASS rbd || {
    echo "ERROR rbd test 2: see above pod \"$pod\" error"; return 1; }

  return 0
}

# execute the requested test(s).
# $@= list of 1 or more tests (space separator)
function execute_tests() {

  local tests="$@"
  local test; local errcnt=0

  echo "*** Executing tests ..."
  echo

  # execute each requested test:
  ##(( GENERAL_TEST )) && {  # don't support general tests for now...
    ##general_test || ((errcnt++)); }

  (( GLUSTER_TEST )) && {
    gluster_test || ((errcnt++)); }

  (( NFS_TEST )) && {
    nfs_test || ((errcnt++)); }

  (( RBD_TEST )) && {
    rbd_test || ((errcnt++)); }

  echo
  echo "***"
  echo "*** Done with tests: $errcnt errors"
  echo "***"
  echo

  (( errcnt > 0 )) && return 1
  return 0
}


## main ##

echo
parse_cmd $@ || exit -1

NS="$(get_current_namespace)"

# set supplemental group and user IDs as globals
get_supplemental_ids $NS || exit 1

validate_ose_env || exit 1

if (( ! VERIFY_ONLY )) ; then
  (( NUM_TESTS > 1 )) && plural='s' || plural=''
  echo "*** Will run $NUM_TESTS test$plural on ose-master \"$MASTER\":"
  for test in ${TESTS[@]}; do
     echo "       $test"
  done
  execute_tests ${TESTS[@]} || exit 1
fi

exit 0
