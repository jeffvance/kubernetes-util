## Openshift Storage Test Script

The [oc-test](oc-test) script can be used to verify/validate an OSE environment, or verify the OSE setup and run one or more storage plugin related test suites. All pods use the busybox container image and the container's mount is always */usr/share/busybox*.

### Contents
  - [Overview](#overview) and usage
  - [Example 1](#example-1-verify-the-target-ose-environment) - verify OSE env only
  - [Example 2](#example-2-verify-ose-env-using-non-official-origin) - verify OSE env only using custom origin
  - [Example 3](#example-3-nfs-test-suite) - NFS tests
  - [Example 4](#example-4-gluster-storage-test-suite) - Gluster tests
  - [Example 5](#example-5-ceph-rbd-test-suite) - Ceph-RBD tests
  - [Example 6](#example-6-all-test-suites) - all tests 

### Overview
At the end of the tests all pods, PV, PVCs, endpoints, secrets, etc. that were successfully created remain actively running. This allows the tester to inspect containers, exec into running containers and test file access, verify the container's user's ids, etc. Over time these types of manual container focused tests could be automated.

There is no attempt to run *docker rm* to delete containers and reclaim their associated storage, so this should be done by the tester.
 
Here's is the full syntax, which is displayed when no arguments are supplied:

```
  oc-test - execute one or more ose storage tests

SYNTAX
  oc-test [test-name] [options]

DESCRIPTION
  Verifies the ose-master environment and, optionally, executes one or more
  storage tests on the ose-master server.

  test-name  A list of one or more tests to run. Expected values are:
               nfs,
               gluster,
               rbd (or ceph),
               all.
             More than one test is specified with a comma separator, eg.
             "nfs,gluster".

  --verify   Only test the ose enviromment but don't run any tests.
  --master <node>
             Hostname or ip of the ose-master node, default is local host
  --oc-prefix <path>
             An optional path prefix appended to all "oc" commands, eg.
             "/root/origin/_output/local/bin/linux/amd64" for local ose builds.
  --sgids <number-list>
             An optional list of 0 or more supplemental group IDs (separated by
             a comma only) to be prepended to the default SGID provided by OSE.
             This list of GIDs only applies to shared storage tests (gluster,
             nfs) since this storage type supports multiple users. For block
             storage (eg. ceph), use the --block-gid option. If --sgids is used,
             each created pod uses these GIDs in its securityContext spec. If
             omitted, the default is to use the first id in the supplemental
             group ID range defined for the current project. See:
               $ oc get ns <project> -o yaml  # for the various IDs
  --block-gid <number>
             An optional supplemental group ID to used as the FSGroup value for
             the created pod. This GID only applies to block storage tests
             (ceph). For shared storage (eg. nfs, gluster), use the --sgids
             option. If --block-gid is specified each created pod uses this GID 
             in its securityContext spec. If omitted, the default is to use the
             first id in the supplemental group ID range defined for the
             current project. See:
               $ oc get ns <project> -o yaml  # for the various IDs
  --nfs-server <hostname|ip>
             NFS server's hostname or ip. Required if performing the nfs tests,
             otherwise ignored.
  --gluster-nodes <node-list>
             A list of 2 or more gluster storage node IP addresses (comma
             separator), which become the ose endpoints. Hostnames are converted
             to IPs using getent, but if DNS or /etc/hosts is not setup
             correctly it is better to supply IPs. Required if performing the
             gluster tests, otherwise ignored. Eg.:
               192.168.122.21,192.168.122.22
  --gluster-vol <volName>
             The name of an existing gluster/RHS volume. Required if performing
             the gluster tests, otherwise ignored.
  --rbd-monitors <node-list>
             A list of 1 or more ceph Monitor IP addresses (comma separated).
             Hostnames are converted to IPs using getent, but if DNS or
             /etc/hosts is not setup correctly it is better to supply the IP.
             If the default port of 6789 is not used then :portNum must follow
             all IPs not using the default port.  Required if performing the 
             block-rbd tests, otherwise ignored. Eg:
               192.168.122.133:6788,192.168.122.134
  --ceph-secret64 <base64 value>
             The base-64 encoded secret string generated by running:
               ceph auth get-key client.admin  , followed by pasting that output
             to the base64 command:
               echo -n "<output-from-ceph-auth-get-key>" | base64
             If omitted and the rbd test is requested and a provided ceph 
             monitor is reachable via ssh, then this value is calculated by the
             rbd test and is not required. It is ignore by all tests other than
             the ceph-rbd tests.
  --rbd-image <name>
             The name of the ceph-rbd image. If the rbd pool is not defaulted to
             "rbd" then the pool name is also required in the form of:
               <pool-name>/<image-name>
             Ignored for all tests other than the ceph-rbd tests.
  --version  Show the version string.
  -q         Suppress "continue?" prompts, skip test environment setup check on
             each ose-node (which also eliminates password prompts on those
             nodes), and reduce instructional output.
```


## Examples:
The examples below show how to run *oc-test* to test gluster, nfs, ceph, or running all tests in one invocation. Note that to run test A and test B, simply specify *A,B* (no space) as the test value.


### Example 1: verify the target OSE environment
In this example "rhel7-ose-1" is the name of the OSE master host.

```
  ./oc-test --verify --master rhel7-ose-1

*** Validating ose-master: "rhel7-ose-1"...

Login successful.

Using project "default".

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default (current)
  * openshift
  * openshift-infra

... validated

=============================================
 OSE master node   : rhel7-ose-1
 OSE nodes         : rhel7-ose-1
 Current project   : "default"
   User IDs        : 12345/10
   Group IDs       : 5550/10,590/5,1000000000/10000
   Default SGID    : 5550
 Additional SGIDs  : 5555,590  (from cmd args)
 Shared storage pod's GID : 5555,590,5550
 Blk storage pod's FSGroup: 5550
=============================================
```


### Example 2: verify OSE env using non-official origin
The ```--oc-prefix``` option allows the tester to build her own version of origin, specifying the path to the oc binary so that the script's *oc* commands use the correct copy of *oc*.

```
./oc-test --verify --master rhel7-ose-1 --oc-prefix /root/origin/_output/local/bin/linux/amd64
```

  
### Example 3: NFS test suite
The NFS setup on all OSE worker nodes is checked to ensure that nfs has been installed properly, that nfs and rpcbind are running, and that the correct selinux booleans are set ON, and that the NFS server is reacheable by all ose-nodes.
The required SE booleans are:
  - nfs_export_all_ro --> on
  - nfs_export_all_rw --> on
  - nfsd_anon_write --> off
  - openshift_use_nfs --> on
  - virt_use_nfs --> on

However, the script only verifies that *openshift_use_nfs* and *virt_use_nfs* are ON, since the other flags seem to be on by default. In this example the NFS server is "f21-nfs".
```
  ./oc-test --master rhel7-ose-1 --nfs-server f21-nfs  nfs

*** Validating ose-master: "rhel7-ose-1"...

Login successful.

Using project "default".

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default (current)
  * openshift
  * openshift-infra

... validated

=============================================
 OSE master node   : rhel7-ose-1
 OSE nodes         : rhel7-ose-1
 Current project   : "default"
   User IDs        : 12345/10
   Group IDs       : 5550/10,590/5,1000000000/10000
   Default SGID    : 5550
 Additional SGIDs  : 5555,590  (from cmd args)
 Shared storage pod's GID : 5555,590,5550
 Blk storage pod's FSGroup: 5550
=============================================

*** Will run 1 test on ose-master "rhel7-ose-1":
       nfs
*** Executing tests ...


*** NFS test suite ***

Connecting to NFS server f21-nfs via ssh.
You may need to enter a password. (possibly more than once)

root@f21-nfs's password: 

    Busybox is run with a volume mounted to the NFS server: f21-nfs.
    Remember to open port 2049 on the NFS server. A good test for this is,
    from the openshift-master, "rhel7-ose-1", execute:
       $ telnet f21-nfs 2049  # ctrl-c to exit
    To open port 2049 execute (on the NFS server):
       $ iptables -I INPUT 1 -p tcp --dport 2049 -j ACCEPT

    Also, on the NFS server, edit /etc/exports to include /opt/nfs, eg:
       /opt/nfs *(rw,sync,no_root_squash)

    The group ID for the NFS export directory /opt/nfs is:
       5555
    and permissions on this directory are:
       drwxrws---.

    To edit the range of supplemental group IDs, on the openshift-master, use:
       $ oc edit ns default
    and change the 'openshift.io/sa.scc.supplemental-groups' range to
    include the NFS group ID for /opt/nfs.  Also, use:
       $ oc get ns default -o yaml
    to see the values for various IDs in the "default" project.

    On the other hand, if it's ok to change the perms on /opt/nfs to match
    openshift's range of groups, then execute (on f21-nfs):
       $ chgrp 5550 /opt/nfs

    Note: it may necessary to restart NFS:
       $ systemctl restart rpcbind nfs

Press any key to continue...
...checking /etc/exports on the NFS server (f21-nfs)...
root@f21-nfs's password: 
...checking that all OSE-nodes are setup for the NFS client...
   (May be prompted for your password on each node, possibly
    multiple times)...

   *** on node: rhel7-ose-1...
...all OSE-nodes are setup for the NFS client

----------
NFS Test 1: baseline: busybox, nfs plugin
  (no supplementalGroups defined for this pod)

... deleting pod "nfs-pod1" (if it exists)...
pod "nfs-pod1" created
... checking pod "nfs-pod1" ...
... checking mount type "nfs" for pod "nfs-pod1" ...
... checking perms of /usr/share/busybox on pod "nfs-pod1" ...

----------
NFS Test 2: busybox, nfs plugin, SGIDs: 5555,590,5550

... deleting pod "nfs-pod2" (if it exists)...
pod "nfs-pod2" created
... checking pod "nfs-pod2" ...
... checking mount type "nfs" for pod "nfs-pod2" ...
... checking perms of /usr/share/busybox on pod "nfs-pod2" ...

----------
NFS Test 3: busybox, PV, PVC, SGIDs: 5555,590,5550

... deleting pvc "nfs-pvc" (if it exists)...
... deleting pv "nfs-pv" (if it exists)...
persistentvolume "nfs-pv" created
... checking PV "nfs-pv" ...
persistentvolumeclaim "nfs-pvc" created
... checking PVC "nfs-pvc" ...
... deleting pod "nfs-pod3" (if it exists)...
pod "nfs-pod3" created
... checking pod "nfs-pod3" ...
... checking mount type "nfs" for pod "nfs-pod3" ...
... checking perms of /usr/share/busybox on pod "nfs-pod3" ...

***
*** Done with tests: 0 errors
***
```
Again `-q` suppresses the continue prompt and the bulk of the nfs instructional output. Also `--sgid` can be supplied to set the supplemental Group IDs in the busybox container, which may be necessary to allow the container to read and/or write to the nfs mount.
  
Here are the newly created pods:
```
$ oc get pods
NAME       READY     STATUS    RESTARTS   AGE
nfs-pod1   1/1       Running   0          14m
nfs-pod2   1/1       Running   0          14m
nfs-pod3   1/1       Running   0          14m
```


### Example 4: Gluster storage test suite
This example runs the gluster tests against an existing gluster cluster and volume. The gluster setup on all OSE worker nodes is checked to ensure that gluster has been installed, and that the correct selinux booleans are set ON. The required SE booleans are:
  - virt_sandbox_use_fusefs --> on
  - virt_sandbox_use_fusefs --> on
```
  ./oc-test --master rhel7-ose-1 --gluster-vol=HadoopVol --gluster-nodes=rhs-1.vm,rhs-2.vm gluster

*** Validating ose-master: "rhel7-ose-1"...

Login successful.

Using project "default".

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default (current)
  * openshift
  * openshift-infra

... validated

=============================================
 OSE master node   : rhel7-ose-1
 OSE nodes         : rhel7-ose-1
 Current project   : "default"
   User IDs        : 12345/10
   Group IDs       : 5550/10,590/5,1000000000/10000
   Default SGID    : 5550
 Additional SGIDs  : 5555,590  (from cmd args)
 Shared storage pod's GID : 5555,590,5550
 Blk storage pod's FSGroup: 5550
=============================================

*** Will run 1 test on ose-master "rhel7-ose-1":
       gluster
*** Executing tests ...


*** Gluster test suite ***

Connecting to 192.168.122.21 via ssh. You may need to enter a password.


    The supplied gluster storage nodes (endpoints) and the glusterfs plugin
    are tested using the busybox container to access the HadoopVol volume.
    On one of the gluster nodes, eg. 192.168.122.21, ensure that gluster is
    running, the "HadoopVol" volume is active, and the volume mount 
    has the correct permissions. Eg:
       $ gluster peer status
       $ gluster volume status HadoopVol
       $ mount | grep glusterfs
       # if HadoopVol is not displayed, then:
       $ mount -a # assuming the vol mount is present in /etc/fstab
       # if the vol mount is not in /etc/fstab, then add it, eg:
       192.168.122.21:/HadoopVol /mnt/glusterfs/HadoopVol glusterfs _netdev 0 0
       
    The group ID for the "HadoopVol" volume mount is:
       590
    and permissions on this volume's mount directory are:
       drwxrwx---

    To edit the range of supplemental group IDs, on "rhel7-ose-1", use:
       $ oc edit ns default
    and change the 'openshift.io/sa.scc.supplemental-groups' range to
    include the HadoopVol's mount's group ID.  Also, use:
       $ oc get ns default -o yaml
    to see the values for various IDs in the "default" project.

    On all of the OSE nodes make sure to:
      $ yum install glusterfs-client
      $ setsebool -P virt_sandbox_use_fusefs 1  # on, add the fusefs label
      $ setenforce 1 # keep selinux enforcing

    **NOTE: the setsebool command above. It is critical for enabling POSIX
            file access from the target containers!!

Press any key to continue...
...checking that all OSE-nodes are setup for the gluster client...
   (May be prompted for your password on each node, possibly
    multiple times)...

   *** on node: rhel7-ose-1...
...all OSE-nodes are setup for the gluster client

----------
Gluster Test 1: baseline: busybox, glusterfs plugin
  (no supplementalGroups defined for this pod)

... deleting endpoint "gluster-endpoints" (if it exists)...
endpoints "gluster-endpoints" created
... checking endpoint "gluster-endpoints" ...
... deleting pod "gluster-pod1" (if it exists)...
pod "gluster-pod1" created
... checking pod "gluster-pod1" ...
... checking mount type "glusterfs" for pod "gluster-pod1" ...
... checking perms of /usr/share/busybox on pod "gluster-pod1" ...

----------
Gluster Test 2: busybox, glusterfs plugin, SGIDs: 5555,590,5550

... deleting pod "gluster-pod2" (if it exists)...
pod "gluster-pod2" created
... checking pod "gluster-pod2" ...
... checking mount type "glusterfs" for pod "gluster-pod2" ...
... checking perms of /usr/share/busybox on pod "gluster-pod2" ...

----------
Gluster Test 3: busybox, PV, PVC, SGIDs: 5555,590,5550

... deleting pvc "gluster-pvc" (if it exists)...
... deleting pv "gluster-pv" (if it exists)...
persistentvolume "gluster-pv" created
... checking PV "gluster-pv" ...
persistentvolumeclaim "gluster-pvc" created
... checking PVC "gluster-pvc" ...
... deleting pod "gluster-pod3" (if it exists)...
pod "gluster-pod3" created
... checking pod "gluster-pod3" ...
... checking mount type "glusterfs" for pod "gluster-pod3" ...
... checking perms of /usr/share/busybox on pod "gluster-pod3" ...

***
*** Done with tests: 0 errors
***
```
And the resulting pods:
```
$ oc get pods
NAME           READY     STATUS    RESTARTS   AGE
gluster-pod1   1/1       Running   0          4m
gluster-pod2   1/1       Running   0          4m
gluster-pod3   1/1       Running   0          4m
```


### Example 5: Ceph-RBD test suite
This example runs the ceph-rbd tests against an existing Ceph cluster, which is defined by a single monitor and an existing rbd image. In this example, the ceph secret is automatically generated from the monitor, but the ```--ceph-secret64``` option can be specified if there is no ssh access to the monitor. Also note the `--block-gid` option, which defines the *FSGroupID* used by the block storage plugins when they "takeover" block storage volumes.
```
./oc-test --master rhel7-ose-1 --rbd-monitors 192.168.122.133 --rbd-image ceph-image rbd
*** Will run 1 test on ose-master "rhel7-ose-1":
       rbd

*** Validating ose-master: "rhel7-ose-1"...

Login successful.

Using project "default".

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default (current)
  * openshift
  * openshift-infra
  
... validated
===================================
 Master node     : rhel7-ose-1
 Current project : default
   Sup User IDs  : 12345/10
   Sup Group IDs : 5555-5555,1000000000/10000
 Supplied Sup GID: <none>
 Pod's Group ID  : 5555
===================================

*** Executing tests ...

*** RBD test suite ***

Calculating base64 ceph secret value...
Connecting to 192.168.122.133 via ssh. You may need to enter a password.

root@192.168.122.133's password: <entered password for ceph mon node>
Computed ceph secret: QVFBOFF2SlZheUJQRVJBQWgvS2cwT1laQUhPQno3akZwekxxdGc9PQ==

    Busybox is run with a volume mounted via the RBD plugin. The ceph user is
    currently hard-coded to "admin" and the file system is assumed to be ext4.
    (Both of these assumptions can easily be removed and added as script args.) 

    Ceph needs to be installed and running correctly. Here are a few tips to 
    perform on one of the monitors (eg. 192.168.122.133):
      rbd create ceph-image -s 1024  # create the image
      rbd map ceph-image             # map the image to the default pool, 'rbd'
      rbd showmapped
      ls /dev/rbd*                   # see which /dev/rbd device is used
      mkfs.ext4 /dev/rbd0            # create the file system
      # provide the below output as the --ceph-secret64 value
      ceph auth get-key client.admin | base64

      # on rhel7-ose-1:
      yum install -y ceph-common

Press any key to continue...

----------
RBD Test 1: baseline: busybox, ceph-rbd plugin:
... deleting secret "ceph-secret" (if it exists)...
secret "ceph-secret" created
... checking secret "ceph-secret" ...
... deleting pod "rbd-pod1" (if it exists)...
pod "rbd-pod1" created
... checking pod "rbd-pod1" ...
... checking mount type "rbd" for pod "rbd-pod1" ...

----------
RBD Test 2: busybox, ceph-rbd plugin, PV, PVC, SGID=5555:
... deleting pv "rbd-pv" (if it exists)...
persistentvolume "rbd-pv" created
... checking PV "rbd-pv" ...
... deleting pvc "rbd-pvc" (if it exists)...
persistentvolumeclaim "rbd-pvc" created
... checking PVC "rbd-pvc" ...
... deleting pod "rbd-pod2" (if it exists)...
pod "rbd-pod2" created
... checking pod "rbd-pod2" ...
... checking mount type "rbd" for pod "rbd-pod2" ...

***
*** Done with tests: 0 errors
***
```

### Example 6: ALL test suites
All tests in a single *oc-test* invocation and somewhat quietly.
```
   ./oc-test --master rhel7-ose-1 --gluster-vol=HadoopVol --gluster-nodes=rhs-1.vm,rhs-2.vm --nfs-server=f21-nfs  --rbd-monitors 192.168.122.133 --rbd-image ceph-image  all -q
   
   *** Will run 3 tests on ose-master "rhel7-ose-1":
       nfs
       gluster
       rbd

*** Validating ose-master: "rhel7-ose-1"...

Login successful.

Using project "default".

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default (current)
  * openshift
  * openshift-infra

... validated

*** Executing tests ...

... (test output deleted) ...

***
*** Done with tests: 0 errors
***
```

And on the OSE-master, here are the pods created by all of the storage tests:
```
$ oc get pods
NAME           READY     STATUS    RESTARTS   AGE
gluster-pod1   1/1       Running   0          3m
gluster-pod2   1/1       Running   0          3m
gluster-pod3   1/1       Running   0          3m
nfs-pod1       1/1       Running   0          2m
nfs-pod2       1/1       Running   0          2m
nfs-pod3       1/1       Running   0          2m
rbd-pod1       1/1       Running   0          1m
rbd-pod2       1/1       Running   0          1m
```

